{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import random as rnd\n",
    "import torch\n",
    "import ast\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_sentences():\n",
    "\n",
    "    # Extract src data\n",
    "    source_data = []\n",
    "    top_data = []\n",
    "    top_decoupled_data = []\n",
    "\n",
    "    # Read source.txt\n",
    "    with open('../dataset/source.txt', 'r') as file:\n",
    "        source_data = ast.literal_eval(file.read())\n",
    "\n",
    "    # Read labels.txt\n",
    "    with open(\"../dataset/input_labels.txt\", 'r') as file:\n",
    "        labels = file.read().splitlines()\n",
    "\n",
    "    return source_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unique_labels(file_path):\n",
    "    labels = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        labels = file.read().splitlines()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dev_dataset(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    pattern_src = r'(?<=\"dev\\.SRC\": \").+(?=\", \"dev\\.EXR\")'\n",
    "    pattern_top = r'(?<=\"dev\\.TOP\": \").+(?=\", \"dev\\.PCFG_ERR\")'\n",
    "    test_src = re.finditer(pattern_src, text)\n",
    "    test_top = re.finditer(pattern_top, text)\n",
    "    test_src_arr = []\n",
    "    test_top_decoupled_arr = []\n",
    "\n",
    "    for match in test_src:\n",
    "        test_src_arr.append(match.group())\n",
    "    \n",
    "    pattern_top_decoupled = r'(?<=\\))[\\w ]*(?= \\()|(?<=ORDER)[\\w ]*(?= \\()|(?<=PIZZAORDER)[\\w ]*(?= \\()|(?<=DRINKORDER)[\\w ]*(?= \\()'\n",
    "    for match in test_top:\n",
    "        temp = re.sub(pattern_top_decoupled,'',match.group())\n",
    "        test_top_decoupled_arr.append(temp)\n",
    "\n",
    "    return test_src_arr, test_top_decoupled_arr\n",
    "\n",
    "#read_dev_dataset(\"../dataset/PIZZA_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tc(train_SRC,train_TOP):\n",
    "    \n",
    "    def parse_sexp(s):\n",
    "        s = s.replace('(', ' ( ').replace(')', ' ) ')\n",
    "        tokens = s.split()\n",
    "        def helper(tokens):\n",
    "            token = tokens.pop(0)\n",
    "            if token == '(':\n",
    "                L = []\n",
    "                while tokens[0] != ')':\n",
    "                    L.append(helper(tokens))\n",
    "                tokens.pop(0)\n",
    "                return L\n",
    "            else:\n",
    "                return token\n",
    "        return helper(tokens.copy())\n",
    "\n",
    "    tree = parse_sexp(train_TOP)\n",
    "\n",
    "    entities = []\n",
    "\n",
    "    def extract_entities(tree, current_label=None, text_accumulator=[]):\n",
    "        if isinstance(tree, list):\n",
    "            label = tree[0]\n",
    "            content = tree[1:]\n",
    "            text = []\n",
    "            for item in content:\n",
    "                extract_entities(item, label, text)\n",
    "            entity_text = ' '.join(text)\n",
    "            #if label in ['ORDER', 'PIZZAORDER', 'NOT'] or label not in ['NUMBER']:\n",
    "            match = re.search(re.escape(entity_text), train_SRC)\n",
    "            if match:\n",
    "                if label == \"NOT\":\n",
    "                    temp_entity = entities.pop()\n",
    "                    entities.append({\n",
    "                    'label': label+\"-\"+temp_entity['label'],\n",
    "                    'word': match.group(),\n",
    "                    })\n",
    "\n",
    "                else:\n",
    "                    entities.append({\n",
    "                        'label': label,\n",
    "                        'word': match.group(),\n",
    "                    })\n",
    "            text_accumulator.extend(text)\n",
    "        else:\n",
    "            text_accumulator.append(tree)\n",
    "\n",
    "    extract_entities(tree)\n",
    "\n",
    "    result = {\n",
    "        'sentence': train_SRC,\n",
    "        'entities': entities\n",
    "    }\n",
    "    #print(result)\n",
    "    return result\n",
    "\n",
    "def generate_bio_tags(sentence, entities):\n",
    "    \n",
    "    words = sentence.split()\n",
    "    bio_tags = [\"0\"] * len(words)  \n",
    "    \n",
    "    \n",
    "    for entity in entities:\n",
    "        label = entity['label'] \n",
    "        entity_words = entity['word'].split()  \n",
    "        \n",
    "        \n",
    "        if label in ['PIZZAORDER', 'ORDER', 'DRINKORDER','COMPLEX_TOPPING']:\n",
    "            continue\n",
    "        \n",
    "        for i in range(len(words)):\n",
    "            if words[i:i+len(entity_words)] == entity_words:\n",
    "                bio_tags[i] = f\"B-{label}\"  \n",
    "                for j in range(1, len(entity_words)):\n",
    "                    bio_tags[i+j] = f\"I-{label}\"  \n",
    "                break  \n",
    "    \n",
    "    return list(zip(words, bio_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage\n",
      "348\n",
      "(ORDER (PIZZAORDER (NUMBER two ) (SIZE medium ) (TOPPING sausage ) (TOPPING black olives ) ) (PIZZAORDER (NUMBER two ) (SIZE medium ) (TOPPING pepperoni ) (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) (PIZZAORDER (NUMBER three ) (SIZE large ) (TOPPING pepperoni ) (TOPPING sausage ) ) )\n",
      "348\n",
      "longest sentence: 33\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongest sentence:\u001b[39m\u001b[38;5;124m\"\u001b[39m,longest_sentence)\n\u001b[0;32m     37\u001b[0m create_test_labels_input()\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_test_labels_input():\n",
    "    longest_sentence = 0\n",
    "    unique_words = set()\n",
    "    result = []\n",
    "    tags = []\n",
    "    ut_labels = read_unique_labels('./unique_labels.txt')\n",
    "    t_labels = {}\n",
    "    t_labels['0'] = 0\n",
    "    for i in range(len(ut_labels)):\n",
    "        t_labels[ut_labels[i]] = i+1\n",
    "    \n",
    "    test_SRC, test_TOP_DECOUPLED = read_dev_dataset(\"../dataset/PIZZA_dev.json\")\n",
    "    test_SRC_size = len(test_SRC)\n",
    "    print(test_SRC[0])\n",
    "    print(len(test_SRC))\n",
    "    print(test_TOP_DECOUPLED[0])\n",
    "    print(len(test_TOP_DECOUPLED))\n",
    "\n",
    "    with open('../dataset/test_input_labels.txt', 'w') as f:\n",
    "        for i in range(test_SRC_size):\n",
    "            test_SRC_item = test_SRC[i]\n",
    "            test_TOP_DECOUPLED_item = test_TOP_DECOUPLED[i]\n",
    "            longest_sentence = max(len(test_SRC_item.split()), longest_sentence)    \n",
    "            unique_words.update(test_SRC_item.split())            \n",
    "\n",
    "            result.append(parse_tc(test_SRC_item,test_TOP_DECOUPLED_item))\n",
    "\n",
    "            tags.append(generate_bio_tags(result[i]['sentence'], result[i]['entities']))\n",
    "\n",
    "            test_SRC_labels_list = []\n",
    "            for word, tag in tags[i]:\n",
    "                test_SRC_labels_list.append(tag)\n",
    "                f.write(f\"{tag} \")\n",
    "            f.write(\"\\n\")\n",
    "    print(\"longest sentence:\",longest_sentence)\n",
    "\n",
    "create_test_labels_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word2vec_embeddings(data, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract Word2Vec embeddings\n",
    "    \"\"\"\n",
    "    sentences = [sentence.split() for sentence in data]\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    # Convert to GPU tensor if possible\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # Get word vectors, use zeros if word not in vocabulary\n",
    "        sent_embedding = [model.wv[word] for word in sentence if word in model.wv] or [np.zeros(100)]\n",
    "        embedding = torch.tensor(np.mean(sent_embedding, axis=0), device=device)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "def extract_contextual_embeddings(data, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract RoBERTa contextual embeddings\n",
    "    \"\"\"\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    for sentence in data:\n",
    "        # Tokenize and move to GPU\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Mean pooling and move back to CPU if needed\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "        embeddings.append(embedding.cpu().numpy())\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def extract_lda_features(data, n_topics=2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(data)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda_features = lda.fit_transform(tfidf_matrix)\n",
    "    return lda_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_word2vec_embeddings(data):\n",
    "#     sentences = [sentence.split() for sentence in data]\n",
    "#     model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "#     embeddings = [np.mean([model.wv[word] for word in sentence if word in model.wv] or [np.zeros(100)], axis=0) for sentence in sentences]\n",
    "#     return np.array(embeddings)\n",
    "\n",
    "\n",
    "# def extract_contextual_embeddings(data):\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#     model = RobertaModel.from_pretrained('roberta-base')\n",
    "#     embeddings = []\n",
    "#     for sentence in data:\n",
    "#         inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#         embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "#     return np.array(embeddings)\n",
    "\n",
    "# def extract_lda_features(data, n_topics=2):\n",
    "#     \"\"\"Extract LDA features.\"\"\"\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = vectorizer.fit_transform(data)\n",
    "#     lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "#     lda_features = lda.fit_transform(tfidf_matrix)\n",
    "#     return lda_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_embedding(data):\n",
    "    word2vec_embeddings = extract_word2vec_embeddings(data)\n",
    "    lda_features = extract_lda_features(data)\n",
    "    contextual_embeddings = extract_contextual_embeddings(data)\n",
    "\n",
    "    word2vec_embeddings = word2vec_embeddings.cpu().numpy()\n",
    "    combined_features = np.hstack((word2vec_embeddings, lda_features, contextual_embeddings))\n",
    "    return combined_features\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, max_len):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    # i guess x should be extended to have the same length as y\n",
    "    self.x_tensor = x\n",
    "    self.y_tensor = torch.tensor([seq + [0] * (max_len - len(seq)) for seq in y], dtype=torch.long)\n",
    "    #################################################################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    ###################### TODO: return the length of the dataset #############################\n",
    "\n",
    "    return len(self.x_tensor)\n",
    "  \n",
    "    ###########################################################################################\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    ###################### TODO: return a tuple of x and y ###################################\n",
    "    return self.x_tensor[idx], self.y_tensor[idx]\n",
    "    ##########################################################################################\n",
    "\n",
    "def save_model(model, epoch, path=\"model_epoch_{}.pth\"):\n",
    "    torch.save(model.state_dict(), path.format(epoch))\n",
    "    print(f\"Model saved to {path.format(epoch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{hours}h {minutes}m {seconds}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimulatedAnnealingLRScheduler:\n",
    "    def __init__(self, \n",
    "                 initial_lr=0.01, \n",
    "                 min_lr=1e-5, \n",
    "                 max_lr=1e-3, \n",
    "                 batch_num=100, \n",
    "                 temperature_init=1.0, \n",
    "                 cooling_rate=0.95):\n",
    "        \"\"\"\n",
    "        Simulated Annealing Learning Rate Scheduler\n",
    "        \n",
    "        Parameters:\n",
    "        - initial_lr: Starting learning rate\n",
    "        - min_lr: Minimum learning rate\n",
    "        - max_lr: Maximum learning rate\n",
    "        - total_epochs: Total training epochs\n",
    "        - temperature_init: Initial temperature\n",
    "        - cooling_rate: Rate of temperature decrease\n",
    "        \"\"\"\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.batch_num = batch_num\n",
    "        self.temperature_init = temperature_init\n",
    "        self.cooling_rate = cooling_rate\n",
    "        \n",
    "        # Dynamic learning rate tracking\n",
    "        self.current_lr = initial_lr\n",
    "    \n",
    "    def get_lr(self, batch_num):\n",
    "        \"\"\"\n",
    "        Compute learning rate based on simulated annealing\n",
    "        \n",
    "        Parameters:\n",
    "        - batch_num: Current batch number\n",
    "        \n",
    "        Returns:\n",
    "        - Adjusted learning rate\n",
    "        \"\"\"\n",
    "        # Calculate current temperature\n",
    "        current_temp = self.temperature_init * (self.cooling_rate ** batch_num)\n",
    "        \n",
    "        # Simulated annealing learning rate calculation\n",
    "        # Smoothly transitions from exploration to exploitation\n",
    "        new_lr = (\n",
    "            self.max_lr * (1-math.exp(-current_temp)) + \n",
    "            self.min_lr * math.exp(-current_temp)\n",
    "        )\n",
    "        \n",
    "        # Ensure learning rate stays within bounds\n",
    "        new_lr = max(self.min_lr, min(new_lr, self.max_lr))\n",
    "        \n",
    "        self.current_lr = new_lr\n",
    "        return new_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model\n",
    "class NER(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_size=512, embedding_dim=768,num_layers=1,dropout=0.1,bidirectional=True,batch_first=True,use_attention=False):\n",
    "        \"\"\"\n",
    "        The constructor of our NER model\n",
    "        Inputs:\n",
    "        - vacab_size: the number of unique words\n",
    "        - embedding_dim: the embedding dimension\n",
    "        - output_dim: output dimension\n",
    "        - hidden_size: the hidden size of the LSTM layer\n",
    "        - num_layers: the number of LSTM layers\n",
    "        - dropout: the dropout rate\n",
    "        - bidirectional: whether to use bidirectional LSTM\n",
    "        \"\"\"\n",
    "        super(NER, self).__init__()\n",
    "        \n",
    "        ## Word embedding layer        \n",
    "        if bidirectional:\n",
    "            hidden_size = hidden_size // 2\n",
    "        # LSTM layer with combined embedding\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size,num_layers=num_layers, \n",
    "                            batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            if bidirectional:\n",
    "                hidden_size *= 2\n",
    "            self.attention = nn.ReLU()\n",
    "        # Linear layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        This function does the forward pass of our model\n",
    "        Inputs:\n",
    "        - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "        Returns:\n",
    "        - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # LSTM and linear layers\n",
    "        if embeddings.dim() == 2:\n",
    "            embeddings = embeddings.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        # Apply attention if needed\n",
    "\n",
    "        if self.use_attention:\n",
    "            # Compute attention weights\n",
    "            attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "            # Apply weighted sum\n",
    "            context = torch.sum(lstm_out * attention_weights, dim=1)\n",
    "        else:\n",
    "            # Use the last output if no attention\n",
    "            context = lstm_out[:, -1, :]\n",
    "\n",
    "        final_output = self.output_layer(context)\n",
    "        \n",
    "        return final_output\n",
    "  \n",
    "def train(model, train_dataset,norm_clip = 0.1, batch_size=512, epochs=5, learning_rate=0.00005 , save_path=\"models_lstm/model_epoch_{}.pth\"):\n",
    "    \"\"\"\n",
    "    This function implements the training logic\n",
    "    Inputs:\n",
    "    - model: the model ot be trained\n",
    "    - train_dataset: the training set of type NERDataset\n",
    "    - batch_size: integer represents the number of examples per step\n",
    "    - epochs: integer represents the total number of epochs (full training pass)\n",
    "    - learning_rate: the learning rate to be used by the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    ############################## TODO: replace the Nones in the following code ##################################\n",
    "    \n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    sa_scheduler = SimulatedAnnealingLRScheduler(\n",
    "        initial_lr=0.1, \n",
    "        min_lr=1e-5, \n",
    "        max_lr=5e-2, \n",
    "        batch_num=len(train_dataloader) * epochs,\n",
    "    )\n",
    "    # (3) create the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=sa_scheduler.initial_lr)\n",
    "    epoch_times = []\n",
    "    batch_times = []\n",
    "\n",
    "    # GPU configuration\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    else:\n",
    "        print(\"CUDA is not available. Training on CPU ...\")\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        start_time = time.time()\n",
    "        batches_prev = epoch_num * len(train_dataloader)\n",
    "        batch_idx = 0\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            current_lr = sa_scheduler.get_lr(batches_prev + batch_idx)\n",
    "        \n",
    "                # Update optimizer learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = current_lr\n",
    "            batch_start_time = time.time()\n",
    "\n",
    "            # (4) move the train input to the device\n",
    "            embeddings = lstm_embedding(train_input)\n",
    "            embeddings_tensor = torch.tensor(embeddings).float()\n",
    "            embeddings_tensor = embeddings_tensor.to(device)\n",
    "\n",
    "            train_label = train_label.float()\n",
    "            train_label = train_label.to(device)\n",
    "            \n",
    "            # (5) move the train label to the device\n",
    "\n",
    "            # (6) do the forward pass\n",
    "            output = model(embeddings_tensor)\n",
    "            # output = output.permute(0, 2, 1) \n",
    "\n",
    "            batch_loss = criterion(output.reshape(-1), train_label.view(-1))\n",
    "\n",
    "            # (8) append the batch loss to the total_loss_train\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "\n",
    "            train_label = train_label.permute(1, 0) \n",
    "            acc = torch.sum(torch.argmax(output, dim=-1) == train_label)\n",
    "            total_acc_train += acc\n",
    "\n",
    "            # (10) zero your gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # (11) do the backward pass\n",
    "            batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), norm_clip) \n",
    "            # (12) update the weights with your optimizer\n",
    "            optimizer.step()\n",
    "            # Calculate batch time\n",
    "            print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {batch_loss.item()} \\\n",
    "            | Train Accuracy: {acc/(train_dataset.y_tensor.size(1)*batch_size)}\\n')\n",
    "            print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            batch_times.append(batch_time)\n",
    "\n",
    "            # Calculate remaining time\n",
    "            avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "            remaining_batches = (epochs - epoch_num - 1) * len(train_dataloader) + (len(train_dataloader) - batch_idx - 1)\n",
    "            remaining_time = avg_batch_time * remaining_batches\n",
    "\n",
    "            # Print batch metrics and remaining time\n",
    "            print(f\"Epoch {epoch_num+1}/{epochs}, Batch {batch_idx+1}/{len(train_dataloader)}\")#, Loss: {loss.item()}, Accuracy: {(predicted == train_label).sum().item() / train_label.numel()}\")\n",
    "            print(f\"Time for batch {batch_idx+1}: {batch_time:.2f} seconds\")\n",
    "            print(f\"Estimated remaining time: {format_time(remaining_time)} seconds\")\n",
    "            batch_idx += 1\n",
    "        ##############################################################################################################    \n",
    "        # epoch loss\n",
    "        epoch_loss = total_loss_train / len(train_dataset)\n",
    "        epoch_acc = total_acc_train / (len(train_dataset) * train_dataset.y_tensor.size(1))\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "            | Train Accuracy: {epoch_acc}\\n')\n",
    "        print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # (13) calculate the accuracy\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        # Calculate remaining time\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_time = avg_epoch_time * (epochs - (epoch_num + 1))\n",
    "\n",
    "        # Print epoch metrics and remaining time\n",
    "        print(f\"Epoch {epoch_num+1}/{epochs}, Loss: {total_loss_train/len(train_dataloader)}\")#, Accuracy: {total_acc_train/len(train_dataloader.dataset)}\")\n",
    "        print(f\"Time for epoch {epoch_num+1}: {epoch_time:.2f} seconds\")\n",
    "        print(f\"Estimated remaining time: {format_time(remaining_time)} seconds\")\n",
    "\n",
    "        save_model(model, epoch_num, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, max_len):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    # i guess x should be extended to have the same length as y\n",
    "    self.x_tensor = x\n",
    "    self.y_tensor = torch.tensor([seq + [0] * (max_len - len(seq)) for seq in y], dtype=torch.long)\n",
    "    #################################################################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    ###################### TODO: return the length of the dataset #############################\n",
    "\n",
    "    return len(self.x_tensor)\n",
    "  \n",
    "    ###########################################################################################\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    ###################### TODO: return a tuple of x and y ###################################\n",
    "    return self.x_tensor[idx], self.y_tensor[idx]\n",
    "    ##########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, path=\"model_epoch_{}.pth\"):\n",
    "    torch.save(model.state_dict(), path.format(epoch))\n",
    "    print(f\"Model saved to {path.format(epoch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, max_len):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    # i guess x should be extended to have the same length as y\n",
    "    self.x_tensor = x\n",
    "    self.y_tensor = torch.tensor([seq + [0] * (max_len - len(seq)) for seq in y], dtype=torch.long)\n",
    "    #################################################################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    ###################### TODO: return the length of the dataset #############################\n",
    "\n",
    "    return len(self.x_tensor)\n",
    "  \n",
    "    ###########################################################################################\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    ###################### TODO: return a tuple of x and y ###################################\n",
    "    return self.x_tensor[idx], self.y_tensor[idx]\n",
    "    ##########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, path=\"model_epoch_{}.pth\"):\n",
    "    torch.save(model.state_dict(), path.format(epoch))\n",
    "    print(f\"Model saved to {path.format(epoch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_SRC read  can i have a large bbq pulled pork\n",
      "output_labels read  0 0 0 B-NUMBER B-SIZE B-TOPPING I-TOPPING I-TOPPING \n",
      "train_SRC length  10000\n",
      "output_labels length  10000\n",
      "checkpoint 0\n",
      "checkpoint 1\n",
      "model created\n",
      "training starting\n",
      "checkpoint 2\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|█         | 1/10 [00:18<02:50, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 655380.0             | Train Accuracy: 0.0013199999229982495\n",
      "\n",
      "Current Learning Rate: 0.031610\n",
      "Epoch 1/2, Batch 1/10\n",
      "Time for batch 1: 18.94 seconds\n",
      "Estimated remaining time: 0h 5m 59s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 20%|██        | 2/10 [00:34<02:16, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 622968.375             | Train Accuracy: 0.0\n",
      "\n",
      "Current Learning Rate: 0.030667\n",
      "Epoch 1/2, Batch 2/10\n",
      "Time for batch 2: 15.70 seconds\n",
      "Estimated remaining time: 0h 5m 11s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 30%|███       | 3/10 [00:50<01:55, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 620950.6875             | Train Accuracy: 0.0\n",
      "\n",
      "Current Learning Rate: 0.029726\n",
      "Epoch 1/2, Batch 3/10\n",
      "Time for batch 3: 15.76 seconds\n",
      "Estimated remaining time: 0h 4m 45s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 40%|████      | 4/10 [01:06<01:37, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 613708.6875             | Train Accuracy: 0.012279999442398548\n",
      "\n",
      "Current Learning Rate: 0.028791\n",
      "Epoch 1/2, Batch 4/10\n",
      "Time for batch 4: 15.80 seconds\n",
      "Estimated remaining time: 0h 4m 24s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 5/10 [01:21<01:18, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 621105.6875             | Train Accuracy: 0.0139999995008111\n",
      "\n",
      "Current Learning Rate: 0.027862\n",
      "Epoch 1/2, Batch 5/10\n",
      "Time for batch 5: 14.96 seconds\n",
      "Estimated remaining time: 0h 4m 3s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 60%|██████    | 6/10 [01:36<01:02, 15.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 598466.875             | Train Accuracy: 0.012480000033974648\n",
      "\n",
      "Current Learning Rate: 0.026941\n",
      "Epoch 1/2, Batch 6/10\n",
      "Time for batch 6: 15.03 seconds\n",
      "Estimated remaining time: 0h 3m 44s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 70%|███████   | 7/10 [01:51<00:46, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 594046.6875             | Train Accuracy: 0.012639999389648438\n",
      "\n",
      "Current Learning Rate: 0.026032\n",
      "Epoch 1/2, Batch 7/10\n",
      "Time for batch 7: 15.39 seconds\n",
      "Estimated remaining time: 0h 3m 27s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 80%|████████  | 8/10 [02:07<00:30, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 593947.1875             | Train Accuracy: 0.01107999961823225\n",
      "\n",
      "Current Learning Rate: 0.025134\n",
      "Epoch 1/2, Batch 8/10\n",
      "Time for batch 8: 15.54 seconds\n",
      "Estimated remaining time: 0h 3m 10s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 90%|█████████ | 9/10 [02:22<00:15, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 602694.625             | Train Accuracy: 0.006120000034570694\n",
      "\n",
      "Current Learning Rate: 0.024251\n",
      "Epoch 1/2, Batch 9/10\n",
      "Time for batch 9: 15.56 seconds\n",
      "Estimated remaining time: 0h 2m 54s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 10/10 [02:38<00:00, 15.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 603589.0             | Train Accuracy: 0.006679999642074108\n",
      "\n",
      "Current Learning Rate: 0.023382\n",
      "Epoch 1/2, Batch 10/10\n",
      "Time for batch 10: 15.31 seconds\n",
      "Estimated remaining time: 0h 2m 37s seconds\n",
      "Epochs: 1 | Train Loss: 612.68578125             | Train Accuracy: 0.007660000119358301\n",
      "\n",
      "Current Learning Rate: 0.023382\n",
      "Epoch 1/2, Loss: 612685.78125\n",
      "Time for epoch 1: 158.05 seconds\n",
      "Estimated remaining time: 0h 2m 38s seconds\n",
      "Model saved to models_lstm/model_epoch_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|█         | 1/10 [00:15<02:21, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 597155.125             | Train Accuracy: 0.012120000086724758\n",
      "\n",
      "Current Learning Rate: 0.022530\n",
      "Epoch 2/2, Batch 1/10\n",
      "Time for batch 1: 15.67 seconds\n",
      "Estimated remaining time: 0h 2m 22s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 20%|██        | 2/10 [00:31<02:07, 15.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 618217.125             | Train Accuracy: 0.0116799995303154\n",
      "\n",
      "Current Learning Rate: 0.021695\n",
      "Epoch 2/2, Batch 2/10\n",
      "Time for batch 2: 16.20 seconds\n",
      "Estimated remaining time: 0h 2m 6s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 30%|███       | 3/10 [00:47<01:51, 15.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 591697.5625             | Train Accuracy: 0.010999999940395355\n",
      "\n",
      "Current Learning Rate: 0.020879\n",
      "Epoch 2/2, Batch 3/10\n",
      "Time for batch 3: 15.82 seconds\n",
      "Estimated remaining time: 0h 1m 50s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 40%|████      | 4/10 [01:04<01:36, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 602491.4375             | Train Accuracy: 0.011039999313652515\n",
      "\n",
      "Current Learning Rate: 0.020081\n",
      "Epoch 2/2, Batch 4/10\n",
      "Time for batch 4: 16.40 seconds\n",
      "Estimated remaining time: 0h 1m 35s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 5/10 [01:22<01:24, 16.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 595035.0625             | Train Accuracy: 0.01152000017464161\n",
      "\n",
      "Current Learning Rate: 0.019304\n",
      "Epoch 2/2, Batch 5/10\n",
      "Time for batch 5: 18.19 seconds\n",
      "Estimated remaining time: 0h 1m 20s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 60%|██████    | 6/10 [01:37<01:05, 16.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 596258.625             | Train Accuracy: 0.013719999231398106\n",
      "\n",
      "Current Learning Rate: 0.018546\n",
      "Epoch 2/2, Batch 6/10\n",
      "Time for batch 6: 15.64 seconds\n",
      "Estimated remaining time: 0h 1m 3s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 70%|███████   | 7/10 [01:53<00:48, 16.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 587473.125             | Train Accuracy: 0.011719999834895134\n",
      "\n",
      "Current Learning Rate: 0.017809\n",
      "Epoch 2/2, Batch 7/10\n",
      "Time for batch 7: 16.03 seconds\n",
      "Estimated remaining time: 0h 0m 47s seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_SRC,output_labels = extract_sentences()\n",
    "train_SRC = train_SRC[:100000]\n",
    "output_labels = output_labels[:10000]\n",
    "print(\"train_SRC read \",train_SRC[0])\n",
    "print(\"output_labels read \",output_labels[0])\n",
    "print(\"train_SRC length \",len(train_SRC))\n",
    "print(\"output_labels length \",len(output_labels))\n",
    "print(\"checkpoint 0\")\n",
    "\n",
    "ut_labels = read_unique_labels('./unique_labels.txt')\n",
    "\n",
    "t_labels = {}\n",
    "t_labels['0'] = 0\n",
    "for i in range(len(ut_labels)):\n",
    "    t_labels[ut_labels[i]] = i+1\n",
    "\n",
    "train_SRC_size = len(train_SRC)\n",
    "\n",
    "longest_sentence = 25\n",
    "print(\"checkpoint 1\")\n",
    "#tag_indices = [[t_labels[tag] for tag in sentence_tags] for sentence_tags in output_labels]\n",
    "for i in range(len(output_labels)):\n",
    "    output_labels[i] = output_labels[i].split()\n",
    "tag_indices = [[t_labels[tag] for tag in sentence_tags] for sentence_tags in output_labels]\n",
    "\n",
    "model = NER(longest_sentence, hidden_size=64, embedding_dim=870,num_layers=1,\n",
    "            dropout=0,bidirectional=True,batch_first=True,use_attention=True)\n",
    "train_dataset = NERDataset(train_SRC, tag_indices, longest_sentence)    \n",
    "print(\"model created\")\n",
    "print(\"training starting\")\n",
    "print(\"checkpoint 2\")\n",
    "print(\"---------------------------------------------\")\n",
    "train(model, train_dataset, batch_size=1000, epochs=2, learning_rate=0.01,norm_clip=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset,batch_size=512, epochs=5):\n",
    "    \"\"\"\n",
    "    This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "    Inputs:\n",
    "    - model: a NER model\n",
    "    - test_dataset: dataset of type NERDataset\n",
    "    \"\"\"    \n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    epoch_times = []\n",
    "    batch_times = []\n",
    "\n",
    "    # GPU configuration\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    else:\n",
    "        print(\"CUDA is not available. Training on CPU ...\")\n",
    "    \n",
    "    total_acc_test = 0\n",
    "  \n",
    "    # (2) disable gradients\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "            # (3) move the test input to the device\n",
    "            test_label = test_label.to(device)\n",
    "\n",
    "            # (4) move the test label to the device\n",
    "            test_input = test_input.to(device)\n",
    "\n",
    "            # (5) do the forward pass\n",
    "            output = model(test_input)\n",
    "\n",
    "            # accuracy calculation (just add the correct predicted items to total_acc_test)\n",
    "            acc =  torch.sum(torch.argmax(output, dim=-1) == test_label).item()\n",
    "\n",
    "            total_acc_test += acc\n",
    "            \n",
    "            # (6) calculate the over all accuracy\n",
    "            \n",
    "            total_acc_test /= (len(test_dataset) * test_dataset.y_tensor.size(1))\n",
    "\n",
    "    print(f'\\nTest Accuracy: {total_acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = NERDataset()\n",
    "evaluate(model, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
