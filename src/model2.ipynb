{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForTokenClassification, RobertaTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_tc(train_SRC,train_TOP):\n",
    "    train_SRC = \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\"\n",
    "    train_TOP = \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING banana pepper ) (TOPPING grilled chicken ) and (TOPPING white onions ) without (NOT (STYLE thin crust ) ) ) )\"\n",
    "\n",
    "    def parse_sexp(s):\n",
    "        s = s.replace('(', ' ( ').replace(')', ' ) ')\n",
    "        tokens = s.split()\n",
    "        def helper(tokens):\n",
    "            token = tokens.pop(0)\n",
    "            if token == '(':\n",
    "                L = []\n",
    "                while tokens[0] != ')':\n",
    "                    L.append(helper(tokens))\n",
    "                tokens.pop(0)\n",
    "                return L\n",
    "            else:\n",
    "                return token\n",
    "        return helper(tokens.copy())\n",
    "\n",
    "    tree = parse_sexp(train_TOP)\n",
    "\n",
    "    entities = []\n",
    "\n",
    "    def extract_entities(tree, current_label=None, text_accumulator=[]):\n",
    "        if isinstance(tree, list):\n",
    "            label = tree[0]\n",
    "            content = tree[1:]\n",
    "            text = []\n",
    "            for item in content:\n",
    "                extract_entities(item, label, text)\n",
    "            entity_text = ' '.join(text)\n",
    "            if label in ['ORDER', 'PIZZAORDER', 'NOT'] or label not in ['NUMBER']:\n",
    "                match = re.search(re.escape(entity_text), train_SRC)\n",
    "                if match:\n",
    "                    entities.append({\n",
    "                        'label': label,\n",
    "                        'word': match.group(),\n",
    "                    })\n",
    "            text_accumulator.extend(text)\n",
    "        else:\n",
    "            text_accumulator.append(tree)\n",
    "\n",
    "    extract_entities(tree)\n",
    "\n",
    "    result = {\n",
    "        'sentence': train_SRC,\n",
    "        'entities': entities\n",
    "    }\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\", 'entities': [{'label': 'TOPPING', 'word': 'banana pepper'}, {'label': 'TOPPING', 'word': 'grilled chicken'}, {'label': 'TOPPING', 'word': 'white onions'}, {'label': 'STYLE', 'word': 'thin crust'}, {'label': 'NOT', 'word': 'thin crust'}, {'label': 'PIZZAORDER', 'word': 'a pizza with banana pepper grilled chicken and white onions without thin crust'}, {'label': 'ORDER', 'word': \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\"}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\",\n",
       " 'entities': [{'label': 'TOPPING', 'word': 'banana pepper'},\n",
       "  {'label': 'TOPPING', 'word': 'grilled chicken'},\n",
       "  {'label': 'TOPPING', 'word': 'white onions'},\n",
       "  {'label': 'STYLE', 'word': 'thin crust'},\n",
       "  {'label': 'NOT', 'word': 'thin crust'},\n",
       "  {'label': 'PIZZAORDER',\n",
       "   'word': 'a pizza with banana pepper grilled chicken and white onions without thin crust'},\n",
       "  {'label': 'ORDER',\n",
       "   'word': \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\"}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_tc(\"\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load RoBERTa-base model and tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=13  # number of NER labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Define a function to preprocess the dataset\n",
    "def preprocess_data(examples):\n",
    "    \"\"\"\n",
    "    Tokenize input text and align labels with tokens.\n",
    "    Handles subwords by assigning -100 to non-aligned tokens.\n",
    "    \"\"\"\n",
    "    print(examples[\"train\"]['train.SRC'])    \n",
    "    tokenized_inputs = tokenizer(examples[\"train\"]['train.SRC'], truncation=True, padding=True, is_split_into_words=True)\n",
    "    print(tokenized_inputs)\n",
    "    labels = []\n",
    "    print(examples[\"label_list\"]['label_list'])\n",
    "    for i, label in enumerate(examples[\"label_list\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100 if (word_idx is None or word_idx == word_ids[j-1]) else label[word_idx] \n",
    "                     for j, word_idx in enumerate(word_ids)]\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['train.SRC', 'train.EXR', 'train.TOP', 'train.TOP-DECOUPLED'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['train.SRC', 'train.EXR', 'train.TOP', 'train.TOP-DECOUPLED'],\n",
      "    num_rows: 3\n",
      "})\n",
      "[\"i'd like a pizza with carrots barbecue pulled pork and cheeseburger without thin crust\", \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\", 'i want one regular pizza without any fried onions']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load the pizza order dataset\n",
    "data_path = \"./test.json\"  \n",
    "try:\n",
    "    data = load_dataset('json', data_files=data_path)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to load dataset from {data_path}: {e}\")\n",
    "\n",
    "print(data)\n",
    "print(data['train'])\n",
    "print(data['train']['train.SRC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONTAINERTYPE', 'ORDER', 'NOT', 'SIZE', 'DRINKORDER', 'TOPPING', 'NUMBER', 'STYLE', 'PIZZAORDER', 'COMPLEX_TOPPING', 'DRINKTYPE', 'VOLUME', 'QUANTITY']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define label mapping\n",
    "labels = []\n",
    "with open(\"unique_labels.txt\", 'r') as file:\n",
    "    labels = file.read()\n",
    "labels = labels[:-1]\n",
    "\n",
    "\n",
    "label_list = labels.split(\"\\n\")\n",
    "print(label_list)\n",
    "\n",
    "num_labels = len(label_list)\n",
    "print(num_labels)\n",
    "\n",
    "model.config.num_labels = num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'd like a pizza with carrots barbecue pulled pork and cheeseburger without thin crust\", \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\", 'i want one regular pizza without any fried onions']\n",
      "{'input_ids': [0, 939, 1017, 101, 10, 9366, 19, 28488, 18906, 2468, 12072, 8, 21629, 3209, 25278, 396, 7174, 22196, 939, 1017, 101, 10, 9366, 19, 23611, 10702, 20346, 5884, 8, 1104, 21568, 396, 7174, 22196, 939, 236, 65, 1675, 9366, 396, 143, 16708, 21568, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['CONTAINERTYPE', 'ORDER', 'NOT', 'SIZE', 'DRINKORDER', 'TOPPING', 'NUMBER', 'STYLE', 'PIZZAORDER', 'COMPLEX_TOPPING', 'DRINKTYPE', 'VOLUME', 'QUANTITY']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m label_list \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: label_list})\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mselect_columns(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.SRC\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mselect_columns(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.TOP-DECOUPLED\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: label_list\n\u001b[0;32m      8\u001b[0m })\n\u001b[1;32m---> 10\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m data\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[47], line 14\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     13\u001b[0m     word_ids \u001b[38;5;241m=\u001b[39m tokenized_inputs\u001b[38;5;241m.\u001b[39mword_ids(batch_index\u001b[38;5;241m=\u001b[39mi)\n\u001b[1;32m---> 14\u001b[0m     label_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (word_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m word_idx \u001b[38;5;241m==\u001b[39m word_ids[j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mlabel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m]\u001b[49m \n\u001b[0;32m     15\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m j, word_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(word_ids)]\n\u001b[0;32m     16\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label_ids)\n\u001b[0;32m     17\u001b[0m tokenized_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Step 5: Split and preprocess the dataset\n",
    "label_list = Dataset.from_dict({\"label_list\": label_list})\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": data[\"train\"].select_columns(\"train.SRC\"),\n",
    "    \"validation\": data[\"train\"].select_columns(\"train.TOP-DECOUPLED\"),\n",
    "    \"label_list\": label_list\n",
    "})\n",
    "\n",
    "data = preprocess_data(data)\n",
    "\n",
    "data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics using sklearn's classification report.\n",
    "    Filters out ignored tokens (-100) from predictions and labels.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n",
    "    true_preds = [[label_list[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "    from sklearn.metrics import classification_report\n",
    "    return classification_report(true_labels, true_preds, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature. The following columns have been ignored: [train.SRC]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 9: Train and save the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed. Saving model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trained_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2193\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2191\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2192\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[1;32m-> 2193\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[0;32m   2195\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Python312\\Lib\\site-packages\\transformers\\trainer.py:993\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    991\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m--> 993\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    995\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collator_with_removed_columns(data_collator, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Python312\\Lib\\site-packages\\transformers\\trainer.py:919\u001b[0m, in \u001b[0;36mTrainer._remove_unused_columns\u001b[1;34m(self, dataset, description)\u001b[0m\n\u001b[0;32m    917\u001b[0m columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature_columns \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcolumn_names]\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo columns in the dataset match the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms forward method signature. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    921\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following columns have been ignored: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ignored_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    922\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    923\u001b[0m     )\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(datasets\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    926\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mset_format(\n\u001b[0;32m    927\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], columns\u001b[38;5;241m=\u001b[39mcolumns, format_kwargs\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    928\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [train.SRC]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "# Step 9: Train and save the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed. Saving model...\")\n",
    "trainer.save_model(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")\n",
    "print(\"Model and tokenizer saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
