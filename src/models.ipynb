{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.0' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# %pip install spacy\n",
    "# import spacy\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# %pip install transformers\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.2.0\n",
      "Uninstalling numpy-2.2.0:\n",
      "  Successfully uninstalled numpy-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.0-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires h5py>=3.11.0, which is not installed.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, which is not installed.\n",
      "scikit-learn 1.6.0 requires threadpoolctl>=3.1.0, which is not installed.\n",
      "tensorboard 2.18.0 requires markdown>=2.6.8, which is not installed.\n",
      "tensorboard 2.18.0 requires protobuf!=4.24.0,>=3.19.6, which is not installed.\n",
      "tensorboard 2.18.0 requires werkzeug>=1.0.1, which is not installed.\n",
      "keras 3.7.0 requires h5py, which is not installed.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.0 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "2.2.0\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ahmadmohsen/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ahmadmohsen/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ahmadmohsen/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ahmadmohsen/.local/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ahmadmohsen/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Reinstall NumPy\n",
    "%pip uninstall -y numpy\n",
    "%pip install numpy\n",
    "\n",
    "# Verify NumPy installation\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "# Install pandas\n",
    "%pip install pandas\n",
    "\n",
    "# Install Matplotlib\n",
    "%pip install numpy pandas matplotlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\"train.SRC\": \"i'd like a pizza with carrots barbecue pulled pork and cheeseburger without thin crust\", \"train.EXR\": \"(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING CARROTS ) (TOPPING BBQ_PULLED_PORK ) (TOPPING CHEESEBURGER ) (NOT (STYLE THIN_CRUST ) ) ) )\", \"train.TOP\": \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING carrots ) (TOPPING barbecue pulled pork ) and (TOPPING cheeseburger ) without (NOT (STYLE thin crust ) ) ) )\", \"train.TOP-DECOUPLED\": \"(ORDER (PIZZAORDER (NUMBER a ) (TOPPING carrots ) (TOPPING barbecue pulled pork ) (TOPPING cheeseburger ) (NOT (STYLE thin crust ) ) ) )\"},\n",
      "    {\"train.SRC\": \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\", \"train.EXR\": \"(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING BANANA_PEPPERS ) (TOPPING GRILLED_CHICKEN ) (TOPPING WHITE_ONIONS ) (NOT (STYLE THIN_CRUST ) ) ) )\", \"train.TOP\": \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING banana pepper ) (TOPPING grilled chicken ) and (TOPPING white onions ) without (NOT (STYLE thin crust ) ) ) )\", \"train.TOP-DECOUPLED\": \"(ORDER (PIZZAORDER (NUMBER a ) (TOPPING banana pepper ) (TOPPING grilled chicken ) (TOPPING white onions ) (NOT (STYLE thin crust ) ) ) )\"},\n",
      "    {\"train.SRC\": \"i want one regular pizza without any fried onions\", \"train.EXR\": \"(ORDER (PIZZAORDER (NUMBER 1 ) (SIZE REGULARSIZE ) (NOT (TOPPING FRIED_ONIONS ) ) ) )\", \"train.TOP\": \"(ORDER i want (PIZZAORDER (NUMBER one ) (SIZE regular ) pizza without any (NOT (TOPPING fried onions ) ) ) )\", \"train.TOP-DECOUPLED\": \"(ORDER (PIZZAORDER (NUMBER one ) (SIZE regular ) (NOT (TOPPING fried onions ) ) ) )\"}\n",
      "]\n",
      "[\"i'd like a pizza with carrots barbecue pulled pork and cheeseburger without thin crust\", \"i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\", 'i want one regular pizza without any fried onions']\n",
      "-------------------\n",
      "[\"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING carrots ) (TOPPING barbecue pulled pork ) and (TOPPING cheeseburger ) without (NOT (STYLE thin crust ) ) ) )\", \"(ORDER i'd like (PIZZAORDER (NUMBER a ) pizza with (TOPPING banana pepper ) (TOPPING grilled chicken ) and (TOPPING white onions ) without (NOT (STYLE thin crust ) ) ) )\", '(ORDER i want (PIZZAORDER (NUMBER one ) (SIZE regular ) pizza without any (NOT (TOPPING fried onions ) ) ) )']\n",
      "-------------------\n",
      "['(ORDER (PIZZAORDER (NUMBER a ) (TOPPING carrots ) (TOPPING barbecue pulled pork ) (TOPPING cheeseburger ) (NOT (STYLE thin crust ) ) ) )', '(ORDER (PIZZAORDER (NUMBER a ) (TOPPING banana pepper ) (TOPPING grilled chicken ) (TOPPING white onions ) (NOT (STYLE thin crust ) ) ) )', '(ORDER (PIZZAORDER (NUMBER one ) (SIZE regular ) (NOT (TOPPING fried onions ) ) ) )']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def read_test_cases(file_path):\n",
    "    \"\"\"\n",
    "    Read test cases from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of test cases.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "        return contents\n",
    "    \n",
    "def preprocess_text(path):\n",
    "    \"\"\"Preprocess text data.\"\"\"\n",
    "    text = read_test_cases(path)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted characters and symbols from text.\"\"\"\n",
    "    pattern = r'(?<=\"train\\.SRC\": \").+(?=\", \"train\\.EXR\")'\n",
    "    train_src = re.finditer(pattern, text)\n",
    "    train_src_arr = []\n",
    "    for match in train_src:\n",
    "        train_src_arr.append(match.group())\n",
    "    return train_src_arr  \n",
    "\n",
    "def preprocess_train_top(text):\n",
    "    \"\"\"Get the training topics from the text.\"\"\"\n",
    "    pattern = r'(?<=\"train\\.TOP\": \").+(?=\", \"train\\.TOP-DECOUPLED\")'\n",
    "    train_top = re.finditer(pattern, text)\n",
    "    train_top_arr = []\n",
    "    for match in train_top:\n",
    "        train_top_arr.append(match.group())\n",
    "    return train_top_arr\n",
    "\n",
    "def preprocess_train_top_decoupled(text):\n",
    "    \"\"\"Get the training topics from the text.\"\"\"\n",
    "    pattern = r'(?<=\"train\\.TOP-DECOUPLED\": \").+(?=\"})'\n",
    "    train_top_decoupled = re.finditer(pattern, text)\n",
    "    train_top_decoupled_arr = []\n",
    "    for match in train_top_decoupled:\n",
    "        train_top_decoupled_arr.append(match.group())\n",
    "    return train_top_decoupled_arr\n",
    "\n",
    "\n",
    "\n",
    "all_data = preprocess_text('test.json')\n",
    "print(all_data)\n",
    "\n",
    "train_src = clean_text(all_data)\n",
    "print(train_src)\n",
    "print('-------------------')\n",
    "train_top = preprocess_train_top(all_data)\n",
    "print(train_top)\n",
    "print('-------------------')\n",
    "train_top_decoupled = preprocess_train_top_decoupled(all_data)\n",
    "print(train_top_decoupled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1:\n",
      "train_src: i'd like a pizza with carrots barbecue pulled pork and cheeseburger without thin crust\n",
      "Tree 1:\n",
      "ORDER\n",
      "    PIZZAORDER\n",
      "        NUMBER\n",
      "            a\n",
      "        TOPPING\n",
      "            cheeseburger\n",
      "        NOT\n",
      "            STYLE\n",
      "                thin\n",
      "                crust\n",
      "{'ORDER': [{'PIZZAORDER': [{'NUMBER': [{'a': None}], 'TOPPING': [{'cheeseburger': None}], 'NOT': [{'STYLE': [{'thin': None, 'crust': None}]}]}]}]}\n",
      "------------------------------\n",
      "Data 2:\n",
      "train_src: i'd like a pizza with banana pepper grilled chicken and white onions without thin crust\n",
      "Tree 2:\n",
      "ORDER\n",
      "    PIZZAORDER\n",
      "        NUMBER\n",
      "            a\n",
      "        TOPPING\n",
      "            white\n",
      "            onions\n",
      "        NOT\n",
      "            STYLE\n",
      "                thin\n",
      "                crust\n",
      "{'ORDER': [{'PIZZAORDER': [{'NUMBER': [{'a': None}], 'TOPPING': [{'white': None, 'onions': None}], 'NOT': [{'STYLE': [{'thin': None, 'crust': None}]}]}]}]}\n",
      "------------------------------\n",
      "Data 3:\n",
      "train_src: i want one regular pizza without any fried onions\n",
      "Tree 3:\n",
      "ORDER\n",
      "    PIZZAORDER\n",
      "        NUMBER\n",
      "            one\n",
      "        SIZE\n",
      "            regular\n",
      "        NOT\n",
      "            TOPPING\n",
      "                fried\n",
      "                onions\n",
      "{'ORDER': [{'PIZZAORDER': [{'NUMBER': [{'one': None}], 'SIZE': [{'regular': None}], 'NOT': [{'TOPPING': [{'fried': None, 'onions': None}]}]}]}]}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def parse_tree_to_hierarchy(tree_str):\n",
    "    \"\"\"\n",
    "    Parse a structured tree string into a hierarchical format (like the visual example).\n",
    "\n",
    "    Args:\n",
    "        tree_str (str): The input parse tree string.\n",
    "\n",
    "    Returns:\n",
    "        dict: A hierarchical representation of the parse tree.\n",
    "    \"\"\"\n",
    "    def recursive_parse(tokens):\n",
    "        tree = {}\n",
    "        while tokens:\n",
    "            token = tokens.pop(0)\n",
    "            if token == \")\":  # End of subtree\n",
    "                return tree\n",
    "            elif token.startswith(\"(\"):  # Start of a new subtree\n",
    "                node = token[1:]  # Remove the '('\n",
    "                tree[node] = []  # Initialize node's children\n",
    "                child = recursive_parse(tokens)\n",
    "                if child:\n",
    "                    tree[node].append(child)\n",
    "            else:\n",
    "                tree[token] = None  # Leaf node (no children)\n",
    "        return tree\n",
    "\n",
    "    # Tokenize the tree string\n",
    "    tokens = tree_str.replace(\"(\", \" (\").replace(\")\", \" )\").split()\n",
    "    return recursive_parse(tokens)\n",
    "\n",
    "def display_tree(tree, level=0):\n",
    "    \"\"\"\n",
    "    Display the hierarchical tree in an indented format.\n",
    "\n",
    "    Args:\n",
    "        tree (dict): The hierarchical tree.\n",
    "        level (int): The current indentation level.\n",
    "    \"\"\"\n",
    "    for key, value in tree.items():\n",
    "        print(\" \" * (level * 4) + f\"{key}\")\n",
    "        if isinstance(value, list) and value:\n",
    "            for child in value:\n",
    "                display_tree(child, level + 1)\n",
    "        elif value is None:  # Leaf node\n",
    "            pass\n",
    "\n",
    "\n",
    "# Convert and Display the Hierarchical Parse Tree\n",
    "for i, tree_str in enumerate(train_top_decoupled):\n",
    "    print(f\"Data {i + 1}:\")\n",
    "    print(f\"train_src: {train_src[i]}\") # Display the training source\n",
    "\n",
    "    print(f\"Tree {i + 1}:\")\n",
    "    tree = parse_tree_to_hierarchy(tree_str) # Parse the tree string\n",
    "    display_tree(tree)\n",
    "    print(tree)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.cli import download\n",
    "\n",
    "# # Download the Spacy model\n",
    "# download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import os\n",
    "\n",
    "# # Define a specific path for nltk data\n",
    "# nltk_data_path = os.path.expanduser(\"~/nltk_data\")\n",
    "# os.makedirs(nltk_data_path, exist_ok=True)\n",
    "\n",
    "# # Force download punkt\n",
    "# nltk.download('punkt', download_dir=nltk_data_path)\n",
    "\n",
    "# # Add the directory to nltk's data path\n",
    "# if nltk_data_path not in nltk.data.path:\n",
    "#     nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# # Debugging: Print the data path\n",
    "# print(\"NLTK Data Path:\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# sample_text = \"I would like a large chicago style pizza.\"\n",
    "# tokens = word_tokenize(sample_text)\n",
    "# print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Download NLTK Tokenizer Resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Spacy Model for Lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your existing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # Step 1: Data Cleaning\n",
    "# ######################################\n",
    "# def clean_text(text):\n",
    "#     \"\"\"\n",
    "#     Clean input text by removing special characters, extra spaces, and standardizing case.\n",
    "#     \"\"\"\n",
    "#     text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "#     text = text.lower()  # Convert to lowercase\n",
    "#     text = text.replace(\"dont\", \"do not\").replace(\"id\", \"i would\")  # Expand contractions\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # Step 2: Tokenization\n",
    "# ######################################\n",
    "# def tokenize_text(text):\n",
    "#     \"\"\"\n",
    "#     Tokenize text into words.\n",
    "#     \"\"\"\n",
    "#     return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # Step 3: Lemmatization (Optional)\n",
    "# ######################################\n",
    "# def lemmatize_text(text):\n",
    "#     \"\"\"\n",
    "#     Perform lemmatization on the text to reduce words to their base forms.\n",
    "#     \"\"\"\n",
    "#     doc = nlp(text)\n",
    "#     return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # Step 4: Entity Annotation Placeholder (Manual or Automated)\n",
    "# ######################################\n",
    "# # Assume pre-labeled dataset is provided for supervised training.\n",
    "# # Add your annotation logic or preprocessing steps for labels here.\n",
    "\n",
    "# def annotate_entities(example_text):\n",
    "#     \"\"\"\n",
    "#     Placeholder for entity annotation.\n",
    "#     Add custom logic or use tools like Prodigy/Label Studio.\n",
    "#     \"\"\"\n",
    "#     # Example Output: Pre-labeled tokens with BIO format\n",
    "#     tokens = tokenize_text(example_text)\n",
    "#     tags = [\"O\"] * len(tokens)  # Replace with actual annotation logic\n",
    "#     return list(zip(tokens, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # Step 5: Convert Data to Model-Friendly Format\n",
    "# ######################################\n",
    "# def prepare_bio_format(data):\n",
    "#     \"\"\"\n",
    "#     Convert text and labels into BIO-tagged format for training.\n",
    "#     \"\"\"\n",
    "#     bio_data = []\n",
    "#     for text in data:\n",
    "#         annotated = annotate_entities(text)\n",
    "#         bio_data.extend(annotated)\n",
    "#     return bio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # Step 6: Dataset Splitting\n",
    "# ######################################\n",
    "# def split_dataset(data):\n",
    "#     \"\"\"\n",
    "#     Split dataset into training, validation, and test sets.\n",
    "#     \"\"\"\n",
    "#     train, temp = train_test_split(data, test_size=0.2, random_state=42)\n",
    "#     val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "#     return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # Step 7: Feature Extraction\n",
    "# ######################################\n",
    "# # Bag of Words (BoW)\n",
    "# def extract_bow_features(texts):\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     return vectorizer.fit_transform(texts)\n",
    "\n",
    "# # TF-IDF\n",
    "# def extract_tfidf_features(texts):\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     return vectorizer.fit_transform(texts)\n",
    "\n",
    "# # Contextual Word Embeddings\n",
    "# def extract_contextual_embeddings(texts):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#     model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "#     embeddings = []\n",
    "#     for text in texts:\n",
    "#         inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         outputs = model(**inputs)\n",
    "#         embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
    "#     return torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# # Main Workflow Example\n",
    "# ######################################\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example Dataset\n",
    "#     dataset = [\n",
    "#         \"I would like a large chicago style pizza with extra cheese and no onions.\",\n",
    "#         \"Two medium cans of diet pepsi and one large bottle of coke.\",\n",
    "#     ]\n",
    "\n",
    "#     # Clean Text\n",
    "#     cleaned_data = [clean_text(text) for text in dataset]\n",
    "\n",
    "#     # Tokenize Text\n",
    "#     tokenized_data = [tokenize_text(text) for text in cleaned_data]\n",
    "\n",
    "#     # Optional Lemmatization\n",
    "#     lemmatized_data = [lemmatize_text(text) for text in cleaned_data]\n",
    "\n",
    "#     # Prepare Data for BIO Tagging\n",
    "#     bio_data = prepare_bio_format(cleaned_data)\n",
    "\n",
    "#     # Split Dataset\n",
    "#     train, val, test = split_dataset(cleaned_data)\n",
    "\n",
    "#     # Extract Features\n",
    "#     bow_features = extract_bow_features(train)\n",
    "#     tfidf_features = extract_tfidf_features(train)\n",
    "#     contextual_features = extract_contextual_embeddings(train)\n",
    "\n",
    "#     print(\"Preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
